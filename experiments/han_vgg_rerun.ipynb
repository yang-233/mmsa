{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'CLS',\n",
       " 'embedding_dim': 200,\n",
       " 'embedding': None,\n",
       " 'freeze_embedding': False,\n",
       " 'word_hidden_size': 50,\n",
       " 'word_layers': 1,\n",
       " 'uniform_bound': 0.1,\n",
       " 'sentence_hidden_size': 50,\n",
       " 'sentence_layers': 1,\n",
       " 'use_imgs': True,\n",
       " 'img_input_size': 4096,\n",
       " 'img_output_size': 100,\n",
       " 'img_num': 3,\n",
       " 'dropout': 0.5,\n",
       " 'output_size': 5,\n",
       " 'bias_init': 1.0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ly/workspace/mmsa\")\n",
    "seed = 1323\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "from models.han_vgg import *\n",
    "from utils.train import *\n",
    "from typing import *\n",
    "from collections import Counter\n",
    "\n",
    "from utils.load_yelp import *\n",
    "from utils.dataset import *\n",
    "from utils.train import *\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"embedding_dim\"] = 100\n",
    "config[\"freeze_embedding\"] = True\n",
    "config[\"word_hidden_size\"] = 100\n",
    "config[\"sentence_hidden_size\"] = 100\n",
    "config[\"use_imgs\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ly/miniconda3/envs/torch1.7/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total': 4667605, 'Trainable': 384205}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No 1 epoch: 554it [01:29,  6.20it/s]\n",
      "No 2 epoch: 554it [01:31,  6.09it/s]\n",
      "No 3 epoch: 554it [01:28,  6.24it/s]\n",
      "No 4 epoch: 554it [01:28,  6.26it/s]\n",
      "No 5 epoch: 554it [01:29,  6.16it/s]\n",
      "No 6 epoch: 554it [01:29,  6.19it/s]\n",
      "No 7 epoch: 554it [01:28,  6.27it/s]\n",
      "No 8 epoch: 554it [01:28,  6.23it/s]\n",
      "No 9 epoch: 554it [01:29,  6.19it/s]\n",
      "No 10 epoch: 554it [01:29,  6.19it/s]\n",
      "No 11 epoch: 554it [01:30,  6.12it/s]\n",
      "No 12 epoch: 554it [01:29,  6.20it/s]\n",
      "No 13 epoch: 554it [01:29,  6.22it/s]\n",
      "No 14 epoch: 554it [01:29,  6.18it/s]\n",
      "No 15 epoch: 554it [01:29,  6.18it/s]\n",
      "/home/ly/miniconda3/envs/torch1.7/lib/python3.8/site-packages/torch/nn/modules/rnn.py:742: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/aten/src/ATen/native/cudnn/RNN.cpp:775.)\n",
      "  result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 12s, sys: 1min 19s, total: 23min 31s\n",
      "Wall time: 23min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.5914221218961625, 0.5880260623255349), 0.9062465807116059)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "train_set, valid_set, test_set= load_glove_vgg_data(split811data, config)\n",
    "train_loader, valid_loader, test_loader = get_loader(batch_size, workers, get_collate_fn(config), train_set, valid_set, test_set)\n",
    "model = Model(config).cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "print(get_parameter_number(model))\n",
    "\n",
    "_interval = 5\n",
    "lr = 1e-3\n",
    "epoches = 50\n",
    "stoping_step = 5\n",
    "optimizer = get_regal_optimizer(model, optim.AdamW, lr)\n",
    "\n",
    "viz = get_Visdom()\n",
    "batch_loss_drawer = VisdomScalar(viz, f\"batch_loss interval:{_interval}\")\n",
    "epoch_loss_drawer = VisdomScalar(viz, f\"Train and valid loss\", 2)\n",
    "acc_drawer = VisdomScalar(viz, \"Train and valid accuracy\", 2)\n",
    "text_writer = VisdomTextWriter(viz, \"Training\")\n",
    "\n",
    "batch_loss = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "\n",
    "res, model = train_visdom_v2(model, optimizer, loss, viz, train_loader,\n",
    "                          valid_loader, epoches, batch_loss, batch_loss_drawer,\n",
    "                          train_loss, valid_loss, epoch_loss_drawer,\n",
    "                          train_acc, valid_acc, acc_drawer, text_writer,\n",
    "                         _interval=_interval, early_stop=stoping_step)\n",
    "eval_model(model, test_loader, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'CLS',\n",
       " 'embedding_dim': 100,\n",
       " 'embedding': None,\n",
       " 'freeze_embedding': True,\n",
       " 'word_hidden_size': 100,\n",
       " 'word_layers': 1,\n",
       " 'uniform_bound': 0.1,\n",
       " 'sentence_hidden_size': 100,\n",
       " 'sentence_layers': 1,\n",
       " 'use_imgs': True,\n",
       " 'img_input_size': 4096,\n",
       " 'img_output_size': 100,\n",
       " 'img_num': 3,\n",
       " 'dropout': 0.5,\n",
       " 'output_size': 5,\n",
       " 'bias_init': 1.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"embedding_dim\"] = 100\n",
    "config[\"freeze_embedding\"] = True\n",
    "config[\"word_hidden_size\"] = 100\n",
    "config[\"sentence_hidden_size\"] = 100\n",
    "config[\"use_imgs\"] = True\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ly/miniconda3/envs/torch1.7/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total': 5077805, 'Trainable': 794405}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No 1 epoch: 554it [01:30,  6.14it/s]\n",
      "No 2 epoch: 554it [01:30,  6.10it/s]\n",
      "No 3 epoch: 554it [01:29,  6.16it/s]\n",
      "No 4 epoch: 554it [01:30,  6.15it/s]\n",
      "No 5 epoch: 554it [01:28,  6.23it/s]\n",
      "No 6 epoch: 554it [01:29,  6.18it/s]\n",
      "No 7 epoch: 554it [01:32,  6.01it/s]\n",
      "No 8 epoch: 554it [01:31,  6.05it/s]\n",
      "No 9 epoch: 554it [01:32,  5.97it/s]\n",
      "No 10 epoch: 554it [01:31,  6.03it/s]\n",
      "No 11 epoch: 554it [01:29,  6.19it/s]\n",
      "No 12 epoch: 554it [01:29,  6.20it/s]\n",
      "No 13 epoch: 554it [01:30,  6.14it/s]\n",
      "No 14 epoch: 554it [01:32,  6.01it/s]\n",
      "No 15 epoch: 554it [01:32,  5.99it/s]\n",
      "/home/ly/miniconda3/envs/torch1.7/lib/python3.8/site-packages/torch/nn/modules/rnn.py:742: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/aten/src/ATen/native/cudnn/RNN.cpp:775.)\n",
      "  result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 32s, sys: 1min 20s, total: 23min 53s\n",
      "Wall time: 23min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.6013544018058691, 0.6032456902461432), 0.907923215049115)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "train_set, valid_set, test_set= load_glove_vgg_data(split811data, config)\n",
    "train_loader, valid_loader, test_loader = get_loader(batch_size, workers, get_collate_fn(config), train_set, valid_set, test_set)\n",
    "model = Model(config).cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "print(get_parameter_number(model))\n",
    "\n",
    "_interval = 5\n",
    "lr = 1e-3\n",
    "epoches = 50\n",
    "stoping_step = 5\n",
    "optimizer = get_regal_optimizer(model, optim.AdamW, lr)\n",
    "\n",
    "viz = get_Visdom()\n",
    "batch_loss_drawer = VisdomScalar(viz, f\"batch_loss interval:{_interval}\")\n",
    "epoch_loss_drawer = VisdomScalar(viz, f\"Train and valid loss\", 2)\n",
    "acc_drawer = VisdomScalar(viz, \"Train and valid accuracy\", 2)\n",
    "text_writer = VisdomTextWriter(viz, \"Training\")\n",
    "\n",
    "batch_loss = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "\n",
    "res, model = train_visdom_v2(model, optimizer, loss, viz, train_loader,\n",
    "                          valid_loader, epoches, batch_loss, batch_loss_drawer,\n",
    "                          train_loss, valid_loss, epoch_loss_drawer,\n",
    "                          train_acc, valid_acc, acc_drawer, text_writer,\n",
    "                         _interval=_interval, early_stop=stoping_step)\n",
    "eval_model(model, test_loader, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.7(py3.8.5)",
   "language": "python",
   "name": "torch1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
