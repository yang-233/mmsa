{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'CLS',\n",
       " 'embedding_dim': 100,\n",
       " 'embedding': None,\n",
       " 'freeze_embedding': True,\n",
       " 'text_hidden_size': 100,\n",
       " 'text_layers': 1,\n",
       " 'bias_init': 1.0,\n",
       " 'uniform_bound': 0.1,\n",
       " 'img_input_size': 2048,\n",
       " 'img_encoder_layers': 1,\n",
       " 'attention_nhead': 4,\n",
       " 'fusion_nheads': 4,\n",
       " 'dropout': 0.1,\n",
       " 'output_size': 3}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ly/workspace/mmsa\")\n",
    "seed = 2245\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "from models.mvsa_lymodel5 import *\n",
    "from utils.train import *\n",
    "from typing import *\n",
    "from utils.load_mvsa import *\n",
    "from utils.dataset import *\n",
    "from utils.train import *\n",
    "from utils.train import *\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ly/miniconda3/envs/mmsa/lib/python3.6/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total': 2272703, 'Trainable': 1013803} CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No 1 epoch: 119it [00:04, 29.22it/s]\n",
      "No 2 epoch: 119it [00:04, 28.72it/s]\n",
      "No 3 epoch: 119it [00:04, 29.05it/s]\n",
      "No 4 epoch: 119it [00:04, 29.60it/s]\n",
      "No 5 epoch: 119it [00:04, 27.08it/s]\n",
      "No 6 epoch: 119it [00:04, 28.74it/s]\n",
      "No 7 epoch: 119it [00:04, 29.66it/s]\n",
      "No 8 epoch: 119it [00:04, 28.96it/s]\n",
      "No 9 epoch: 119it [00:04, 28.05it/s]\n",
      "No 10 epoch: 119it [00:04, 28.20it/s]\n",
      "No 11 epoch: 119it [00:04, 28.05it/s]\n",
      "No 12 epoch: 119it [00:04, 28.16it/s]\n",
      "No 13 epoch: 119it [00:04, 28.36it/s]\n",
      "No 14 epoch: 119it [00:03, 29.89it/s]\n",
      "No 15 epoch: 119it [00:04, 28.62it/s]\n",
      "No 16 epoch: 119it [00:04, 28.17it/s]\n",
      "No 17 epoch: 119it [00:04, 27.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 13.2 s, total: 1min 16s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_set, valid_set, test_set= load_glove_data(config)\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "train_loader, valid_loader, test_loader = get_loader(batch_size, workers, get_collate_fn(config), train_set, valid_set, test_set)\n",
    "model = Model(config).cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print(get_parameter_number(model), loss)\n",
    "\n",
    "_interval = 5\n",
    "lr = 1e-3\n",
    "epoches = 50\n",
    "stoping_step = 10\n",
    "optimizer = get_regal_optimizer(model, optim.AdamW, lr)\n",
    "\n",
    "viz = get_Visdom()\n",
    "batch_loss_drawer = VisdomScalar(viz, f\"batch_loss interval:{_interval}\")\n",
    "epoch_loss_drawer = VisdomScalar(viz, f\"Train and valid loss\", 2)\n",
    "acc_drawer = VisdomScalar(viz, \"Train and valid accuracy\", 2)\n",
    "text_writer = VisdomTextWriter(viz, \"Training\")\n",
    "\n",
    "batch_loss = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "\n",
    "res, model = train_visdom_v2(model, optimizer, loss, viz, train_loader,\n",
    "                          valid_loader, epoches, batch_loss, batch_loss_drawer,\n",
    "                          train_loss, valid_loss, epoch_loss_drawer,\n",
    "                          train_acc, valid_acc, acc_drawer, text_writer,\n",
    "                         _interval=_interval, early_stop=stoping_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_valid_loss': 0.6923315652120015,\n",
       " 'min_valid_loss_epoch': 7,\n",
       " 'min_loss_train_acc': 0.696969696969697,\n",
       " 'min_valid_loss_train_loss': 0.6418053135127954,\n",
       " 'min_loss_valid_acc': 0.6645494243747518,\n",
       " 'last_valid_acc': 0.628423977768956,\n",
       " 'last_train_acc': 0.8900357284636761,\n",
       " 'last_epoch': 17,\n",
       " 'last_train_loss': 0.27591546585166427,\n",
       " 'last_valid_loss': 1.2659579533913534}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ly/miniconda3/envs/mmsa/lib/python3.6/site-packages/torch/nn/modules/rnn.py:738: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427286/work/aten/src/ATen/native/cudnn/RNN.cpp:1234.)\n",
      "  self.num_layers, self.dropout, self.training, self.bidirectional)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.6623067776456599, 0.6348683900974827), 0.699613732853155)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, test_loader, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'CLS',\n",
       " 'embedding_dim': 50,\n",
       " 'embedding': None,\n",
       " 'freeze_embedding': True,\n",
       " 'text_hidden_size': 50,\n",
       " 'text_layers': 1,\n",
       " 'bias_init': 1.0,\n",
       " 'uniform_bound': 0.1,\n",
       " 'img_input_size': 2048,\n",
       " 'img_encoder_layers': 1,\n",
       " 'attention_nhead': 4,\n",
       " 'fusion_nheads': 4,\n",
       " 'dropout': 0.1,\n",
       " 'output_size': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## 调参\n",
    "config[\"embedding_dim\"] = 50\n",
    "config[\"text_hidden_size\"] = 50\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ly/miniconda3/envs/mmsa/lib/python3.6/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total': 986353, 'Trainable': 356903} CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No 1 epoch: 119it [00:04, 29.15it/s]\n",
      "No 2 epoch: 119it [00:04, 29.75it/s]\n",
      "No 3 epoch: 119it [00:04, 28.12it/s]\n",
      "No 4 epoch: 119it [00:04, 27.74it/s]\n",
      "No 5 epoch: 119it [00:04, 28.45it/s]\n",
      "No 6 epoch: 119it [00:04, 29.34it/s]\n",
      "No 7 epoch: 119it [00:04, 27.62it/s]\n",
      "No 8 epoch: 119it [00:04, 28.84it/s]\n",
      "No 9 epoch: 119it [00:04, 27.99it/s]\n",
      "No 10 epoch: 119it [00:03, 30.24it/s]\n",
      "No 11 epoch: 119it [00:04, 28.25it/s]\n",
      "No 12 epoch: 119it [00:04, 28.46it/s]\n",
      "No 13 epoch: 119it [00:04, 27.61it/s]\n",
      "No 14 epoch: 119it [00:04, 27.87it/s]\n",
      "No 15 epoch: 119it [00:04, 29.52it/s]\n",
      "No 16 epoch: 119it [00:04, 28.82it/s]\n",
      "No 17 epoch: 119it [00:04, 28.97it/s]\n",
      "No 18 epoch: 119it [00:04, 29.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 14.1 s, total: 1min 20s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_set, valid_set, test_set= load_glove_data(config)\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "train_loader, valid_loader, test_loader = get_loader(batch_size, workers, get_collate_fn(config), train_set, valid_set, test_set)\n",
    "model = Model(config).cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print(get_parameter_number(model), loss)\n",
    "\n",
    "_interval = 5\n",
    "lr = 1e-3\n",
    "epoches = 50\n",
    "stoping_step = 10\n",
    "optimizer = get_regal_optimizer(model, optim.AdamW, lr)\n",
    "\n",
    "viz = get_Visdom()\n",
    "batch_loss_drawer = VisdomScalar(viz, f\"batch_loss interval:{_interval}\")\n",
    "epoch_loss_drawer = VisdomScalar(viz, f\"Train and valid loss\", 2)\n",
    "acc_drawer = VisdomScalar(viz, \"Train and valid accuracy\", 2)\n",
    "text_writer = VisdomTextWriter(viz, \"Training\")\n",
    "\n",
    "batch_loss = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "\n",
    "res, model = train_visdom_v2(model, optimizer, loss, viz, train_loader,\n",
    "                          valid_loader, epoches, batch_loss, batch_loss_drawer,\n",
    "                          train_loss, valid_loss, epoch_loss_drawer,\n",
    "                          train_acc, valid_acc, acc_drawer, text_writer,\n",
    "                         _interval=_interval, early_stop=stoping_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ly/miniconda3/envs/mmsa/lib/python3.6/site-packages/torch/nn/modules/rnn.py:738: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427286/work/aten/src/ATen/native/cudnn/RNN.cpp:1234.)\n",
      "  self.num_layers, self.dropout, self.training, self.bidirectional)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.6686484344034879, 0.635191072368477), 0.700040311932422)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, test_loader, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'CLS',\n",
       " 'embedding_dim': 25,\n",
       " 'embedding': Embedding(12589, 25),\n",
       " 'freeze_embedding': True,\n",
       " 'text_hidden_size': 25,\n",
       " 'text_layers': 1,\n",
       " 'bias_init': 1.0,\n",
       " 'uniform_bound': 0.1,\n",
       " 'img_input_size': 2048,\n",
       " 'img_encoder_layers': 1,\n",
       " 'attention_nhead': 5,\n",
       " 'fusion_nheads': 5,\n",
       " 'dropout': 0.1,\n",
       " 'output_size': 3,\n",
       " 'vocab_size': 12488}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## 调参\n",
    "config[\"embedding_dim\"] = 25\n",
    "config[\"text_hidden_size\"] = 25\n",
    "config[\"attention_nhead\"] = 5\n",
    "config[\"fusion_nheads\"] = 5\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total': 455678, 'Trainable': 140953} CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No 1 epoch: 119it [00:03, 29.95it/s]\n",
      "No 2 epoch: 119it [00:03, 29.76it/s]\n",
      "No 3 epoch: 119it [00:04, 29.18it/s]\n",
      "No 4 epoch: 119it [00:03, 29.98it/s]\n",
      "No 5 epoch: 119it [00:04, 28.64it/s]\n",
      "No 6 epoch: 119it [00:03, 30.42it/s]\n",
      "No 7 epoch: 119it [00:04, 29.49it/s]\n",
      "No 8 epoch: 119it [00:04, 29.55it/s]\n",
      "No 9 epoch: 119it [00:03, 29.79it/s]\n",
      "No 10 epoch: 119it [00:04, 29.12it/s]\n",
      "No 11 epoch: 119it [00:03, 30.37it/s]\n",
      "No 12 epoch: 119it [00:03, 29.92it/s]\n",
      "No 13 epoch: 119it [00:04, 28.38it/s]\n",
      "No 14 epoch: 119it [00:04, 29.12it/s]\n",
      "No 15 epoch: 119it [00:04, 29.35it/s]\n",
      "No 16 epoch: 119it [00:04, 29.29it/s]\n",
      "No 17 epoch: 119it [00:04, 28.34it/s]\n",
      "No 18 epoch: 119it [00:04, 28.22it/s]\n",
      "No 19 epoch: 119it [00:04, 29.11it/s]\n",
      "No 20 epoch: 119it [00:04, 28.39it/s]\n",
      "No 21 epoch: 119it [00:04, 28.18it/s]\n",
      "No 22 epoch: 119it [00:04, 28.86it/s]\n",
      "No 23 epoch: 119it [00:04, 29.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 17.8 s, total: 1min 39s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_set, valid_set, test_set= load_glove_data(config)\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "train_loader, valid_loader, test_loader = get_loader(batch_size, workers, get_collate_fn(config), train_set, valid_set, test_set)\n",
    "model = Model(config).cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print(get_parameter_number(model), loss)\n",
    "\n",
    "_interval = 5\n",
    "lr = 1e-3\n",
    "epoches = 50\n",
    "stoping_step = 10\n",
    "optimizer = get_regal_optimizer(model, optim.AdamW, lr)\n",
    "\n",
    "viz = get_Visdom()\n",
    "batch_loss_drawer = VisdomScalar(viz, f\"batch_loss interval:{_interval}\")\n",
    "epoch_loss_drawer = VisdomScalar(viz, f\"Train and valid loss\", 2)\n",
    "acc_drawer = VisdomScalar(viz, \"Train and valid accuracy\", 2)\n",
    "text_writer = VisdomTextWriter(viz, \"Training\")\n",
    "\n",
    "batch_loss = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "\n",
    "res, model = train_visdom_v2(model, optimizer, loss, viz, train_loader,\n",
    "                          valid_loader, epoches, batch_loss, batch_loss_drawer,\n",
    "                          train_loss, valid_loss, epoch_loss_drawer,\n",
    "                          train_acc, valid_acc, acc_drawer, text_writer,\n",
    "                         _interval=_interval, early_stop=stoping_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ly/miniconda3/envs/mmsa/lib/python3.6/site-packages/torch/nn/modules/rnn.py:738: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427286/work/aten/src/ATen/native/cudnn/RNN.cpp:1234.)\n",
      "  self.num_layers, self.dropout, self.training, self.bidirectional)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.6539833531510107, 0.6227874402594281), 0.7092597050370085)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, test_loader, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'CLS',\n",
       " 'embedding_dim': 50,\n",
       " 'embedding': None,\n",
       " 'freeze_embedding': True,\n",
       " 'text_hidden_size': 100,\n",
       " 'text_layers': 1,\n",
       " 'bias_init': 1.0,\n",
       " 'uniform_bound': 0.1,\n",
       " 'img_input_size': 2048,\n",
       " 'img_encoder_layers': 1,\n",
       " 'attention_nhead': 4,\n",
       " 'fusion_nheads': 4,\n",
       " 'dropout': 0.1,\n",
       " 'output_size': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## 调参\n",
    "config[\"embedding_dim\"] = 50\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ly/miniconda3/envs/mmsa/lib/python3.6/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total': 1613253, 'Trainable': 983803} CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No 1 epoch: 119it [00:04, 29.34it/s]\n",
      "No 2 epoch: 119it [00:04, 28.46it/s]\n",
      "No 3 epoch: 119it [00:04, 28.10it/s]\n",
      "No 4 epoch: 119it [00:04, 28.77it/s]\n",
      "No 5 epoch: 119it [00:04, 29.36it/s]\n",
      "No 6 epoch: 119it [00:04, 28.00it/s]\n",
      "No 7 epoch: 119it [00:04, 28.79it/s]\n",
      "No 8 epoch: 119it [00:04, 29.00it/s]\n",
      "No 9 epoch: 119it [00:04, 28.39it/s]\n",
      "No 10 epoch: 119it [00:04, 28.33it/s]\n",
      "No 11 epoch: 119it [00:04, 27.61it/s]\n",
      "No 12 epoch: 119it [00:04, 27.71it/s]\n",
      "No 13 epoch: 119it [00:04, 27.86it/s]\n",
      "No 14 epoch: 119it [00:04, 29.29it/s]\n",
      "No 15 epoch: 119it [00:04, 27.62it/s]\n",
      "No 16 epoch: 119it [00:04, 28.26it/s]\n",
      "No 17 epoch: 119it [00:04, 26.90it/s]\n",
      "No 18 epoch: 119it [00:04, 28.00it/s]\n",
      "No 19 epoch: 119it [00:04, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 14.9 s, total: 1min 25s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_set, valid_set, test_set= load_glove_data(config)\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "train_loader, valid_loader, test_loader = get_loader(batch_size, workers, get_collate_fn(config), train_set, valid_set, test_set)\n",
    "model = Model(config).cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print(get_parameter_number(model), loss)\n",
    "\n",
    "_interval = 5\n",
    "lr = 1e-3\n",
    "epoches = 50\n",
    "stoping_step = 10\n",
    "optimizer = get_regal_optimizer(model, optim.AdamW, lr)\n",
    "\n",
    "viz = get_Visdom()\n",
    "batch_loss_drawer = VisdomScalar(viz, f\"batch_loss interval:{_interval}\")\n",
    "epoch_loss_drawer = VisdomScalar(viz, f\"Train and valid loss\", 2)\n",
    "acc_drawer = VisdomScalar(viz, \"Train and valid accuracy\", 2)\n",
    "text_writer = VisdomTextWriter(viz, \"Training\")\n",
    "\n",
    "batch_loss = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "\n",
    "res, model = train_visdom_v2(model, optimizer, loss, viz, train_loader,\n",
    "                          valid_loader, epoches, batch_loss, batch_loss_drawer,\n",
    "                          train_loss, valid_loss, epoch_loss_drawer,\n",
    "                          train_acc, valid_acc, acc_drawer, text_writer,\n",
    "                         _interval=_interval, early_stop=stoping_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ly/miniconda3/envs/mmsa/lib/python3.6/site-packages/torch/nn/modules/rnn.py:738: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427286/work/aten/src/ATen/native/cudnn/RNN.cpp:1234.)\n",
      "  self.num_layers, self.dropout, self.training, self.bidirectional)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.6714229092350377, 0.65578134708461), 0.6907877804880524)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, test_loader, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.7(py3.8.5)",
   "language": "python",
   "name": "torch1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
