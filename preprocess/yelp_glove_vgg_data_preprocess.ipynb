{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "import sys\n",
    "sys.path.append(\"/home/ly/workspace/mmsa\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import bigru\n",
    "from utils.tokenization import BasicTokenizer\n",
    "\n",
    "seed = 1024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open(\"data/yelp-vistanet/clear_data.pickle\", \"rb\") as r:\n",
    "        return pickle.load(r)\n",
    "class YelpSimpleTokenizer(BasicTokenizer):\n",
    "    def __init__(self, vocab:Dict[str, int]=None, do_lower_case:bool=True) -> None:\n",
    "        super(YelpSimpleTokenizer, self).__init__(do_lower_case)\n",
    "        self.SENT_DELIMITER = '|||'\n",
    "        self.vocab = vocab\n",
    "        self.UNK = len(vocab) + 1 if vocab is not None else None # \n",
    "\n",
    "    def tokenize(self, text:str) -> List[str]: # 默认切成2d\n",
    "        res = []\n",
    "        for sent in text.split(self.SENT_DELIMITER):\n",
    "            if len(sent) > 0: # 有一定几率出现空字符串\n",
    "                res.append(super(YelpSimpleTokenizer, self).tokenize(sent))\n",
    "        return res\n",
    "\n",
    "    def _getidx(self, token:str):\n",
    "        return self.vocab.get(token, self.UNK)\n",
    "        \n",
    "    def to_idx(self, text:str) -> List[int]:\n",
    "        assert self.vocab is not None, \"No vocab!\"\n",
    "        sents = self.tokenize(text)\n",
    "        res = []\n",
    "        for sent in sents:\n",
    "            res.append([self._getidx(token) for token in sent])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_freq(reviews:List[dict], freq_dict:Dict[str, int]=None) -> Dict[str, int]: \n",
    "    # 统计词频\n",
    "    tokenizer = YelpSimpleTokenizer(do_lower_case=True)\n",
    "    if freq_dict is None:\n",
    "        freq_dict = {}\n",
    "    for review in tqdm(reviews, \"Count word frequency\"):\n",
    "        text = review[\"Text\"]\n",
    "        for sent in tokenizer.tokenize(text):\n",
    "            for token in sent:\n",
    "                freq_dict[token] = freq_dict.get(token, 0) + 1\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Count word frequency: 100%|██████████| 35445/35445 [00:58<00:00, 610.79it/s]\n",
      "Count word frequency: 100%|██████████| 4430/4430 [00:07<00:00, 616.02it/s]\n"
     ]
    }
   ],
   "source": [
    "freq_dict = count_word_freq(data[\"train\"])\n",
    "freq_dict = count_word_freq(data[\"valid\"], freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_file(path:str):\n",
    "    token2idx = collections.OrderedDict()\n",
    "    idx2token = []\n",
    "    idx = 0\n",
    "    with open(path, \"r\") as r:\n",
    "        for line in tqdm(r):\n",
    "            key = line.strip()\n",
    "            idx2token.append(key)\n",
    "            token2idx[key] = idx\n",
    "            idx += 1\n",
    "    return token2idx, idx2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:00, 1236673.22it/s]\n"
     ]
    }
   ],
   "source": [
    "token2idx, idx2token = load_vocab_file(\"pretrained/glove6B/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_glove(freq_dict:Dict[str, int]):\n",
    "    glove_dict, _ = load_vocab_file(os.path.join(\"pretrained\", \"glove6B\", \"vocab.txt\"))\n",
    "    _vocab = list(filter(lambda item: item[0] in glove_dict, freq_dict.items())) # 删除掉不在glove中的词\n",
    "    _vocab = sorted(_vocab, key=lambda item: item[1], reverse=True) # 降序排序\n",
    "    print(f\"There are {len(_vocab)} words in vocab.\")\n",
    "    token2idx = collections.OrderedDict()\n",
    "    glove_idx = []\n",
    "    idx2token = [\"[PAD]\"]\n",
    "    idx = 1\n",
    "    for key, val in _vocab:\n",
    "        token2idx[key] = idx\n",
    "        idx2token.append(key)\n",
    "        glove_idx.append(glove_dict[key]) # 用来读取glove词向量\n",
    "        idx += 1\n",
    "    idx2token.append(\"[UNK]\")\n",
    "    return token2idx, idx2token, glove_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:00, 1267564.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42822 words in vocab.\n"
     ]
    }
   ],
   "source": [
    "token2idx, idx2token, glove_idx = build_vocab_from_glove(freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(token2idx, idx2token, glove_idx):\n",
    "    vals = {\"token2idx\" : token2idx,\n",
    "           \"idx2token\" : idx2token,\n",
    "           \"glove_idx\" : glove_idx}\n",
    "    path = os.path.join(base_dir, \"glove_vocab.pickle\")\n",
    "    with open(path, \"wb\") as o:\n",
    "        pickle.dump(vals, o, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(token2idx, idx2token, glove_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vocab():\n",
    "    path = os.path.join(base_dir, \"glove_vocab.pickle\")\n",
    "    with open(path, \"rb\") as r:\n",
    "        return pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_glove_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_weight(d:int):\n",
    "    path = os.path.join(\"pretrained\", \"glove6B\", \"glove.6B.\" + str(d) + \"d.txt\")\n",
    "    NUM = 400000\n",
    "    weight = np.empty((NUM, d), dtype=np.float32)\n",
    "    with open(path, \"r\", encoding='utf-8') as r:\n",
    "        for i, line in enumerate(tqdm(r.readlines(), \"Load glove\")):\n",
    "            values = line.split()\n",
    "            weight[i] = np.asarray(values[1:], dtype=np.float32)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42822, 42824, 42822)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab[\"token2idx\"]), len(vocab[\"idx2token\"]), len(vocab[\"glove_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_glove_weight(d:int, _uniform:float=0.1):\n",
    "    path = os.path.join(base_dir, \"glove6B\" + str(d) + \"d.npy\")\n",
    "    if os.path.exists(path):\n",
    "        return np.load(path)\n",
    "    glove_weight = load_glove_weight(d)\n",
    "    vocab = load_glove_vocab()\n",
    "    n = len(vocab[\"token2idx\"]) \n",
    "    weight = np.empty((n + 12, d), dtype=np.float32) \n",
    "    weight[0] = np.zeros(d, dtype=np.float32) # [PAD]\n",
    "    glove_weight = glove_weight[vocab[\"glove_idx\"]]\n",
    "    weight[1:n+1] = glove_weight # 正文\n",
    "    weight[n+1] = glove_weight.mean(axis=0) # [UNK]\n",
    "    weight[n+2:] = np.random.uniform(-_uniform, _uniform, size=(10, d))\n",
    "    np.save(path, weight)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load glove: 100%|██████████| 400000/400000 [00:04<00:00, 95054.50it/s] \n",
      "Load glove: 100%|██████████| 400000/400000 [00:07<00:00, 52383.67it/s]\n",
      "Load glove: 100%|██████████| 400000/400000 [00:12<00:00, 30890.69it/s]\n",
      "Load glove: 100%|██████████| 400000/400000 [00:18<00:00, 21389.14it/s]\n"
     ]
    }
   ],
   "source": [
    "all_d = [50, 100, 200, 300]\n",
    "for d in all_d:\n",
    "    w = get_yelp_glove_weight(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_tokenizer = YelpSimpleTokenizer(vocab[\"token2idx\"], do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_tokenizer.to_idx(data[\"train\"][0][\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg_features(i): # 事实上所有review都只有三张图\n",
    "    path = os.path.join(base_dir, \"raw\", \"photo_features\", i[:2], i + \".npy\")\n",
    "    if os.path.exists(path):\n",
    "        return np.load(path)\n",
    "    else:\n",
    "        return None\n",
    "def build_glove_and_vgg_data(tokenizer, reviews:List[dict]):\n",
    "    res = []\n",
    "    for review in tqdm(reviews):\n",
    "        d = {}\n",
    "        d[\"Text\"] = tokenizer.to_idx(review[\"Text\"])\n",
    "        d[\"Photos\"] = []\n",
    "        for _id in review[\"Photos\"]:\n",
    "            features = load_vgg_features(_id)\n",
    "            if features is not None:\n",
    "                d[\"Photos\"].append(features)\n",
    "        d[\"Rating\"] = review[\"Rating\"]\n",
    "        res.append(d)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35445/35445 [01:24<00:00, 420.35it/s]\n",
      "100%|██████████| 4430/4430 [00:10<00:00, 430.21it/s]\n",
      "100%|██████████| 4430/4430 [00:10<00:00, 415.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 42s, sys: 3.21 s, total: 1min 45s\n",
      "Wall time: 1min 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glove_vgg_data = {}\n",
    "for key in [\"train\", \"valid\", \"test\"]:\n",
    "    glove_vgg_data[key] = build_glove_and_vgg_data(glove_tokenizer, data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/yelp-vistanet/glove_vgg_data.pickle'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vgg_data_path = os.path.join(base_dir, \"glove_vgg_data.pickle\")\n",
    "glove_vgg_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(glove_vgg_data_path, \"wb\") as w:\n",
    "    pickle.dump(glove_vgg_data, w, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44305"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_num = []\n",
    "for key in [\"train\", \"valid\", \"test\"]:\n",
    "    for review in glove_vgg_data[key]:\n",
    "        imgs_num.append(len(review[\"Photos\"]))\n",
    "len(imgs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(imgs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44305"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a == 3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch1.6.0]",
   "language": "python",
   "name": "conda-env-torch1.6.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
