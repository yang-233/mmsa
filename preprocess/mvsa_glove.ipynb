{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir = \"data/mvsa/MVSA-multiple/\"\n",
    "data_path = _dir + \"data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12599"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = None\n",
    "with open(data_path, \"r\") as r:\n",
    "    data = json.load(r)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_glove_vocab():\n",
    "    vocab = OrderedDict()\n",
    "    with open(\"pretrained/glove27b/glove.twitter.27B.25d.txt\", \"r\") as r:\n",
    "        for i, l in enumerate(r.readlines()):\n",
    "            l = l.strip().split()\n",
    "            vocab[l[0]] = i\n",
    "    return vocab\n",
    "glove_vocab = build_glove_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<user>', 0), ('.', 1), (':', 2), ('rt', 3), (',', 4)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glove_vocab.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1629"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_vocab = set()\n",
    "for i in glove_vocab.keys():\n",
    "    if len(i) >= 2 and i.startswith(\"#\"):\n",
    "        topic_vocab.add(i)\n",
    "len(topic_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = re.compile(r\"&[a-zA-Z]+;\") # 清除转义字符\n",
    "p2 = re.compile(r\"([\\W])\") # 用于切分字符串 \n",
    "def clear_text(text):\n",
    "    res = []\n",
    "    split_text = text.strip().lower().split()\n",
    "    for i in split_text:\n",
    "        if i.startswith(\"http\") or p1.match(i) is not None: # 忽略URL和转义字符\n",
    "            continue\n",
    "        elif i.startswith(\"@\"): # \n",
    "            res.append(i[1:])\n",
    "        elif i.startswith(\"#\"):\n",
    "            if len(i) >= 2:\n",
    "                if i in topic_vocab: # 在词表中 则直接加入\n",
    "                    res.append(i)\n",
    "                else: # 否则拆分\n",
    "                    res.append(\"#\")\n",
    "                    res.append(i[1:])\n",
    "            else:\n",
    "                res.append(i)\n",
    "        else: # 其他类型切分然后加入结果中\n",
    "            i = p2.split(i)\n",
    "            for _i in i:\n",
    "                if len(_i) > 0: # 会有空字符串 忽略\n",
    "                    res.append(_i)\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in data:\n",
    "    i[\"text\"] = clear_text(i[\"text\"])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7557, 2519, 2523)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_data = {}\n",
    "point = len(data) // 5\n",
    "glove_data[\"train\"] = data[:point*3]\n",
    "glove_data[\"valid\"] = data[point*3:point*4]\n",
    "glove_data[\"test\"] = data[point*4:]\n",
    "len(glove_data[\"train\"]), len(glove_data[\"valid\"]), len(glove_data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22796"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_freq(freq, data):\n",
    "    for i in data:\n",
    "        for j in i[\"text\"]:\n",
    "            freq[j] = freq.get(j, 0) + 1\n",
    "    return freq\n",
    "freq = {}\n",
    "build_freq(freq, glove_data[\"train\"])\n",
    "build_freq(freq, glove_data[\"valid\"])\n",
    "len(freq) # all 26273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12488"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab_from_glove(freq_dict, glove_vocab, dir):\n",
    "    _vocab = list(filter(lambda item: item[0] in glove_vocab, freq_dict.items())) # 删除掉不在glove中的词\n",
    "    _vocab = sorted(_vocab, key=lambda item: item[1], reverse=True) # 降序排序\n",
    "    token2idx = OrderedDict()\n",
    "    glove_idx = []\n",
    "    idx = 1\n",
    "    for key, val in _vocab:\n",
    "        token2idx[key] = idx\n",
    "        glove_idx.append(glove_vocab[key]) # 用来读取glove词向量\n",
    "        idx += 1\n",
    "    d = {}\n",
    "    d[\"token2idx\"] = token2idx\n",
    "    d[\"glove_idx\"] = glove_idx\n",
    "    with open(dir + \"glove_vocab.pickle\", \"wb\") as o:\n",
    "        pickle.dump(d, o, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return d\n",
    "vocab = build_vocab_from_glove(freq, glove_vocab, _dir) # 221.4kb\n",
    "len(vocab[\"token2idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vocab(_dir):\n",
    "    with open(_dir + \"glove_vocab.pickle\", \"rb\") as r:\n",
    "        return pickle.load(r)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12588\n"
     ]
    }
   ],
   "source": [
    "UNK_NUM = 100\n",
    "class GloveTokenizer:\n",
    "    def __init__(self, glove_vocab, unk_num:int=UNK_NUM):\n",
    "        self.vocab = glove_vocab\n",
    "        self.vocab_size = len(glove_vocab)\n",
    "        self.unk_num = unk_num\n",
    "        print(self.vocab_size + unk_num)\n",
    "    def tokenize(self, tokens_list):\n",
    "        res = []\n",
    "        for i in tokens_list:\n",
    "            if i in self.vocab:\n",
    "                res.append(self.vocab[i])\n",
    "            else:\n",
    "                res.append(random.randint(self.vocab_size + 1, self.vocab_size + self.unk_num))\n",
    "        return res\n",
    "tokenizer = GloveTokenizer(vocab[\"token2idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1193514/1193514 [00:42<00:00, 28140.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12589, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_glove_weight(d:int):\n",
    "    p = re.compile(r\"\\s\")\n",
    "    path = os.path.join(\"pretrained\", \"glove27b\", \"glove.twitter.27B.\" + str(d) + \"d.txt\")\n",
    "    with open(path, \"r\") as r:\n",
    "        file = r.readlines()\n",
    "    n = len(file)\n",
    "    weight = np.zeros((n, d), dtype=np.float32)\n",
    "    for i, line in enumerate(tqdm(file)):\n",
    "        values = p.split(line.strip())\n",
    "        if len(values) == d:\n",
    "            weight[i] = np.asarray(values, dtype=np.float32)\n",
    "        else:\n",
    "            weight[i] = np.asarray(values[1:], dtype=np.float32)\n",
    "    return weight\n",
    "\n",
    "def get_mvsa_glove_weight(dir, d:int, _uniform:float=0.1):\n",
    "    path = os.path.join(dir, \"glove27b\" + str(d) + \"d.npy\")\n",
    "    if os.path.exists(path):\n",
    "        return np.load(path)\n",
    "    glove_weight = load_glove_weight(d)\n",
    "    vocab = load_glove_vocab(dir)\n",
    "    n = len(vocab[\"token2idx\"]) \n",
    "    weight = np.zeros((n + UNK_NUM + 1, d), dtype=np.float32) \n",
    "    weight[1:n+1] = glove_weight[vocab[\"glove_idx\"]] # 正文\n",
    "    glove_size = len(glove_weight)\n",
    "    for i in range(UNK_NUM):\n",
    "        temp_weight = glove_weight[random.sample(list(range(glove_size)), 100000)]\n",
    "        weight[n + i + 1] = temp_weight.mean(axis=0) # UNK\n",
    "    np.save(path, weight) # all 5.4mb\n",
    "    return weight\n",
    "weight = get_mvsa_glove_weight(_dir, 100)\n",
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [\"train\", \"valid\", \"test\"]:\n",
    "    for i in glove_data[key]:\n",
    "        i[\"text\"] = tokenizer.tokenize(i[\"text\"])\n",
    "with open(_dir + \"glove_data.pickle\", \"wb\") as o:\n",
    "    pickle.dump(glove_data, o, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '11401',\n",
       "  'text': [27,\n",
       "   7,\n",
       "   22,\n",
       "   244,\n",
       "   364,\n",
       "   1,\n",
       "   12519,\n",
       "   524,\n",
       "   6,\n",
       "   3097,\n",
       "   6093,\n",
       "   16,\n",
       "   365,\n",
       "   46,\n",
       "   12492,\n",
       "   16,\n",
       "   12525,\n",
       "   2510,\n",
       "   2,\n",
       "   1,\n",
       "   12533],\n",
       "  'label': 'neutral'},\n",
       " {'id': '10832',\n",
       "  'text': [320,\n",
       "   1,\n",
       "   12540,\n",
       "   1,\n",
       "   3098,\n",
       "   1,\n",
       "   3099,\n",
       "   1,\n",
       "   1079,\n",
       "   1,\n",
       "   1080,\n",
       "   1,\n",
       "   12565,\n",
       "   1,\n",
       "   12537,\n",
       "   1,\n",
       "   12505],\n",
       "  'label': 'neutral'},\n",
       " {'id': '8336',\n",
       "  'text': [24,\n",
       "   600,\n",
       "   39,\n",
       "   875,\n",
       "   6094,\n",
       "   1467,\n",
       "   116,\n",
       "   6095,\n",
       "   525,\n",
       "   1,\n",
       "   12503,\n",
       "   159,\n",
       "   27,\n",
       "   9,\n",
       "   47,\n",
       "   76,\n",
       "   4],\n",
       "  'label': 'positive'},\n",
       " {'id': '13991',\n",
       "  'text': [287,\n",
       "   9,\n",
       "   2150,\n",
       "   280,\n",
       "   175,\n",
       "   662,\n",
       "   16,\n",
       "   20,\n",
       "   54,\n",
       "   6,\n",
       "   366,\n",
       "   550,\n",
       "   12570,\n",
       "   12525,\n",
       "   1,\n",
       "   12556],\n",
       "  'label': 'positive'},\n",
       " {'id': '18352',\n",
       "  'text': [12531,\n",
       "   6096,\n",
       "   12556,\n",
       "   15,\n",
       "   12530,\n",
       "   15,\n",
       "   12524,\n",
       "   3,\n",
       "   3,\n",
       "   1,\n",
       "   72,\n",
       "   1,\n",
       "   237,\n",
       "   1,\n",
       "   6097,\n",
       "   1,\n",
       "   12528,\n",
       "   1,\n",
       "   12512,\n",
       "   1,\n",
       "   1867,\n",
       "   1,\n",
       "   12565],\n",
       "  'label': 'neutral'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_data[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mvsa_glove_data():\n",
    "    with open(_dir + \"glove_data.pickle\", \"rb\") as r:\n",
    "        return pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmsa",
   "language": "python",
   "name": "mmsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
