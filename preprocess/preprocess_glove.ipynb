{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "import sys\n",
    "sys.path.append(\"/home/ly/workspace/mmsa\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from utils.tokenization import BasicTokenizer\n",
    "from utils.load_yelp import *\n",
    "seed = 1024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join(\"data\",\"yelp-vistanet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open(\"data/yelp-vistanet/clear_data.pickle\", \"rb\") as r:\n",
    "        return pickle.load(r)\n",
    "class YelpSimpleTokenizer(BasicTokenizer):\n",
    "    def __init__(self, vocab:Dict[str, int]=None, do_lower_case:bool=True) -> None:\n",
    "        super(YelpSimpleTokenizer, self).__init__(do_lower_case)\n",
    "        self.SENT_DELIMITER = '|||'\n",
    "        self.vocab = vocab\n",
    "        self.UNK = len(vocab) + 1 if vocab is not None else None # \n",
    "\n",
    "    def tokenize(self, text:str) -> List[str]: # 默认切成2d\n",
    "        res = []\n",
    "        for sent in text.split(self.SENT_DELIMITER):\n",
    "            if len(sent) > 0: # 有一定几率出现空字符串\n",
    "                res.append(super(YelpSimpleTokenizer, self).tokenize(sent))\n",
    "        return res\n",
    "\n",
    "    def _getidx(self, token:str):\n",
    "        return self.vocab.get(token, self.UNK)\n",
    "        \n",
    "    def to_idx(self, text:str) -> List[int]:\n",
    "        assert self.vocab is not None, \"No vocab!\"\n",
    "        sents = self.tokenize(text)\n",
    "        res = []\n",
    "        for sent in sents:\n",
    "            res.append([self._getidx(token) for token in sent])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42822, 42824, 42822)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data()\n",
    "vocab = load_glove_vocab()\n",
    "glove_tokenizer = YelpSimpleTokenizer(vocab[\"token2idx\"], do_lower_case=True)\n",
    "len(vocab[\"token2idx\"]), len(vocab[\"idx2token\"]), len(vocab[\"glove_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_photo(i): \n",
    "    path = os.path.join(base_dir, \"photos\", i[:2], i + \".jpg\")\n",
    "    return os.path.exists(path)\n",
    "\n",
    "def build_glove_data(tokenizer, reviews:List[dict]):\n",
    "    res = []\n",
    "    total_img = 0\n",
    "    for review in tqdm(reviews):\n",
    "        d = {}\n",
    "        d[\"Text\"] = tokenizer.to_idx(review[\"Text\"])\n",
    "        d[\"Photos\"] = []\n",
    "        for _id in review[\"Photos\"]:\n",
    "            if check_photo(_id):\n",
    "                d[\"Photos\"].append(_id)\n",
    "                total_img += 1\n",
    "        d[\"Rating\"] = review[\"Rating\"]\n",
    "        res.append(d)\n",
    "    print(f\"Image num : {total_img}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35445/35445 [01:23<00:00, 423.84it/s]\n",
      "  1%|▏         | 57/4430 [00:00<00:07, 561.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image num : 132715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4430/4430 [00:08<00:00, 508.59it/s]\n",
      "  1%|▏         | 64/4430 [00:00<00:06, 635.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image num : 16408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4430/4430 [00:08<00:00, 551.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image num : 16304\n",
      "CPU times: user 1min 25s, sys: 1.58 s, total: 1min 27s\n",
      "Wall time: 1min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glove_data = {}\n",
    "for key in [\"train\", \"valid\", \"test\"]:\n",
    "    glove_data[key] = build_glove_data(glove_tokenizer, data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(base_dir, \"glove_data.pickle\")\n",
    "with open(path, \"wb\") as w:\n",
    "    pickle.dump(glove_data, w, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch1.6.0]",
   "language": "python",
   "name": "conda-env-torch1.6.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
